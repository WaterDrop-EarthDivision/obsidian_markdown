---
{"dg-publish":true,"permalink":"/pytorch/early-stopping/","dgPassFrontmatter":true,"created":"2025-10-24T20:01:53.704+08:00","updated":"2025-12-26T14:58:37.101+08:00"}
---


## ç¤ºä¾‹
```python
early_stopping = EarlyStopping(patience=20, verbose=False) Â # è®¾ç½®æ—©åœ Â å¦‚æœéªŒè¯æŸå¤±åœ¨è¿ç»­ 20 è½®å†…æ²¡æœ‰ä¸‹é™ï¼Œåˆ™åœæ­¢è®­ç»ƒã€‚

# å½“å‰ç´¯è®¡æ‰€æœ‰ step çš„å¹³å‡è®­ç»ƒæŸå¤± è¦ä¿å­˜çš„æ¨¡å‹ æ¨¡å‹ä¿å­˜è·¯å¾„/æ–‡ä»¶åå‰ç¼€
early_stopping(np.mean(total_loss), Student.rnn, 'AMP_rnn_topk') Â # è¿™é‡Œæœ‰é—®é¢˜ Â éœ€è¦ä¼ å…¥éªŒè¯é›†çš„æŸå¤± æˆ–è€…ä½¿ç”¨æœ€è¿‘ä¸€æ®µåŒºé—´çš„å¹³å‡æŸå¤±

if early_stopping.early_stop:
	print("Early stopping")
	break
```


## å®Œæ•´ä»£ç 

```python
import gzip
import pickle
import time
import torch
import argparse
import numpy as np
from torch.utils.data import Dataset
from tqdm import tqdm
from transformers import BertTokenizer
from torch.nn import CrossEntropyLoss
import torch.nn.functional as Ffrom AMP_distiilation import RNN
from utils import calculate_likelihood_loss, top_k_top_p_filtering, unique, decode, prompt_model_loader, \
Â  Â  Variable
from early_stop.pytorchtools import EarlyStopping
from soft_prompt_embedding import SoftEmbedding

def setup_args():
Â  Â  parser = argparse.ArgumentParser(
Â  Â  parser.add_argument('--batch_size', default=64, type=int, required=False, help='batch size')
Â  Â  parser.add_argument('--lr', default=1e-3, type=float, required=False, help='learn rate')
Â  Â  parser.add_argument('--alpha', default=1, type=int, required=False, help='soft loss')
Â  Â  # | --alpha | 1 | int | è½¯æŸå¤±æƒé‡ç³»æ•°ï¼Œç”¨äºåŠ æƒè’¸é¦æŸå¤±ï¼ˆKL æ•£åº¦ï¼‰å’Œç¡¬æŸå¤±ï¼ˆå¦‚ CEï¼‰ã€‚
Â  Â  # ä½†åœ¨ä½ çš„ä»£ç ä¸­ï¼š
Â  Â  # loss = soft_loss * args.alpha
Â  Â  # å› ä¸º alpha=1ï¼Œæ‰€ä»¥åªç”¨äº†è½¯æŸå¤±ï¼ˆKL æ•£åº¦ï¼‰ï¼Œæ²¡æœ‰ç¡¬ä»»åŠ¡æŸå¤±ã€‚
Â  Â  # ğŸ“Œ å¦‚æœä½ æƒ³åšæ··åˆæŸå¤±ï¼Œåº”æ”¹ä¸º float ç±»å‹ï¼Œæ¯”å¦‚ alpha=0.7 |
Â  Â  # parser.add_argument('--max_grad_norm', default=1.0, type=float, required=False)
Â  Â  parser.add_argument('--top_k', default=10, type=int, required=False, help='top k sampling') Â # ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œåªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ªè¯ä¸­é‡‡æ ·
Â  Â  parser.add_argument('--top_p', default=1.0, type=float, required=False, help='top p sampling') Â # ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œåªä»ç´¯è®¡æ¦‚ç‡è¶…è¿‡ P çš„è¯ä¸­é‡‡æ · å¸¸ä¸ top_k äºŒé€‰ä¸€ä½¿ç”¨ï¼Œå…¸å‹å€¼ï¼š0.9, 0.95 | Â ç›¸å½“äºä¸é™åˆ¶
Â  Â  parser.add_argument('--sigma', default=15, type=float, required=False, help='the weight of score') Â #
Â  Â  parser.add_argument('--n_steps', default=10000, type=float, required=False, help='the weight of score') Â # è®­ç»ƒæ€»æ­¥æ•°
Â  Â  parser.add_argument('--n_tokens', default=10, type=int, required=False, help='soft embedding') # è½¯æç¤ºï¼ˆSoft Promptï¼‰çš„é•¿åº¦ï¼Œå³åœ¨è¾“å…¥å‰æ·»åŠ å¤šå°‘ä¸ªå¯å­¦ä¹ çš„è¿ç»­å‘é‡ã€‚
Â  Â  return parser.parse_args()
Â  Â  
def sample(args, model, tokenizer, batch_size, text=""):
Â  Â  device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
Â  Â  # model, _ = load_model(args.save_model_path, args.vocab_path)
Â  Â  model.to(device)
Â  Â  model.eval()
Â  Â  time1 = time.time()
Â  Â  max_length = 34
Â  Â  input_ids = list(range(100, 100 + 10))
Â  Â  input_ids.extend(tokenizer.encode(text))
Â  Â  input_ids = input_ids[:11]
Â  Â  input_tensor = torch.zeros(batch_size, 11).long()
Â  Â  for index, i in enumerate(input_ids):
Â  Â  Â  Â  input_tensor[:, index] = input_ids[index]
Â  Â  Seq_list = []
Â  Â  finished = torch.zeros(batch_size, 1).byte().to(device)
Â  Â  log_probs = Variable(torch.zeros(batch_size))
Â  Â  log_probs_list = []
Â  Â  sequences = []
Â  Â  for i in range(max_length):
Â  Â  Â  Â  # input_tensor = torch.tensor([input_ids])
Â  Â  Â  Â  inputs = {"input_ids": input_tensor.to(device)}
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  outputs = model(**inputs)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(e)
Â  Â  Â  Â  logits = outputs.logits
Â  Â  Â  Â  logits = top_k_top_p_filtering(logits, top_k=args.top_k, top_p=args.top_p)
Â  Â  Â  Â  prob = F.softmax(logits[:, -1, :])
Â  Â  Â  Â  log_prob = F.log_softmax(logits[:, -1, :], dim=1)
Â  Â  Â  Â  last_token_id = torch.multinomial(prob, 1)
Â  Â  Â  Â  sequences.append(last_token_id.view(-1, 1))
Â  Â  Â  Â  # Flatten the tokens
Â  Â  Â  Â  log_probs_list.append(log_prob)
Â  Â  Â  Â  # .detach().to('cpu').numpy()
Â  Â  Â  Â  EOS_sampled = (last_token_id == tokenizer.sep_token_id)
Â  Â  Â  Â  finished = torch.ge(finished + EOS_sampled, 1)
Â  Â  Â  Â  if torch.prod(finished) == 1:
Â  Â  Â  Â  Â  Â  # print('End')
Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  # last_token = tokenizer.convert_ids_to_tokens(last_token_id)
Â  Â  Â  Â  input_tensor = torch.cat((input_tensor, last_token_id.detach().to('cpu')), 1)
Â  Â  Â  Â  # Seq_list.append(last_token)
Â  Â  sequences = torch.cat(sequences, 1)
Â  Â  # sequences = sequences.detach()
Â  Â  # Seq_list = np.array(Seq_list).T
Â  Â  return sequences.data, log_probs_list


if __name__ == '__main__':
Â  Â  # set_seed(42)
Â  Â  start_time = time.time()
Â  Â  train_losses = []
Â  Â  total_step = 0
Â  Â  total_loss = []
Â  Â  early_stopping = EarlyStopping(patience=20, verbose=False) Â # è®¾ç½®æ—©åœ Â å¦‚æœéªŒè¯æŸå¤±åœ¨è¿ç»­ 20 è½®å†…æ²¡æœ‰ä¸‹é™ï¼Œåˆ™åœæ­¢è®­ç»ƒã€‚
Â  Â  args = setup_args()
Â  Â  n_steps = args.n_steps
Â  Â  args.model_bin_path, args.vocab_path = './model_ckpt/pytorch_model.bin', '../voc/vocab.txt'
Â  Â  args.model_path = './model_ckpt/'
Â  Â  tokenizer = BertTokenizer(vocab_file=args.vocab_path)
Â  Â  Teacher = prompt_model_loader(args.model_bin_path, args.model_path)
Â  Â  Student = RNN(tokenizer)
Â  Â  # We dont need gradients with respect to Prior
Â  Â  for param in Teacher.parameters():
Â  Â  Â  Â  param.requires_grad = False
Â  Â  # Agent.transformer.wte.learned_embedding.requires_grad = False
Â  Â  optimizer = torch.optim.Adam(Student.rnn.parameters(), lr=args.lr)
Â  Â  soft_criterion = torch.nn.KLDivLoss()
Â  Â  # window_loss = []
Â  Â  for step in range(n_steps):
Â  Â  Â  Â  # Sample from Agent
Â  Â  Â  Â  seqs, agent_likelihood = sample(args, Teacher, tokenizer, batch_size=args.batch_size)
Â  Â  Â  Â  # Remove duplicates, ie only consider unique seqs
Â  Â  Â  Â  # unique_idxs = unique(seqs)
Â  Â  Â  Â  # seqs = seqs[unique_idxs]
Â  Â  Â  Â  # agent_likelihood = agent_likelihood[unique_idxs]
Â  Â  Â  Â  # Get prior likelihood and score
Â  Â  Â  Â  prior_likelihood, hard_loss = Student.likelihood(seqs)
Â  Â  Â  Â  soft_loss = soft_criterion(torch.cat(agent_likelihood, 1), torch.cat(prior_likelihood, 1))
Â  Â  Â  Â  # Calculate loss
Â  Â  Â  Â  # loss = soft_loss * args.alpha + hard_loss * (1 - args.alpha)
Â  Â  Â  Â  loss = soft_loss
Â  Â  Â  Â  # Calculate gradients and make an update to the network weights
Â  Â  Â  Â  optimizer.zero_grad()
Â  Â  Â  Â  loss.backward()
Â  Â  Â  Â  optimizer.step()
Â  Â  Â  Â  total_loss.append(loss.item())
Â  Â  Â  Â  if step % 100 == 0 and step != 0:
Â  Â  Â  Â  Â  Â  # print(optim.state_dict()['param_groups'][0]['lr'])
Â  Â  Â  Â  Â  Â  # decrease_learning_rate(optim, decrease_by=0.03)
Â  Â  Â  Â  Â  Â  tqdm.write("*" * 50)
Â  Â  Â  Â  Â  Â  tqdm.write("step {:3d} Â  Â loss: {:5.2f}\n".format(step, loss.item()))
Â  Â  Â  Â  Â  Â  # mean_loss = np.mean(window_loss) Â  # è®¡ç®—çª—å£å†…çš„å¹³å‡æŸå¤±ä¼šæ›´åŠ åˆç† ä¸ç„¶ä¼šå‡ºç°è¿™ç§æƒ…å†µ
Â  Â  Â  Â  Â  Â  # window_loss = [] Â # æ¸…ç©ºçª—å£
Â  Â  Â  Â  Â  Â  # early_stopping(mean_loss, Student.rnn, 'AMP_rnn_topk')
Â  Â  Â  Â  Â  Â  # if early_stopping.early_stop:
Â  Â  Â  Â  Â  Â  # Â  Â  print("Early stopping triggered!")
Â  Â  Â  Â  Â  Â  # Â  Â  break
Â  Â  Â  Â  # å½“å‰ç´¯è®¡æ‰€æœ‰ step çš„å¹³å‡è®­ç»ƒæŸå¤± è¦ä¿å­˜çš„æ¨¡å‹ æ¨¡å‹ä¿å­˜è·¯å¾„/æ–‡ä»¶åå‰ç¼€
Â  Â  Â  Â  early_stopping(np.mean(total_loss), Student.rnn, 'AMP_rnn_topk') Â # è¿™é‡Œæœ‰é—®é¢˜ Â éœ€è¦ä¼ å…¥éªŒè¯é›†çš„æŸå¤± æˆ–è€…ä½¿ç”¨æœ€è¿‘ä¸€æ®µåŒºé—´çš„å¹³å‡æŸå¤±
Â  Â  Â  Â  if early_stopping.early_stop:
Â  Â  Â  Â  Â  Â  print("Early stopping")
Â  Â  Â  Â  Â  Â  break

# Step Â å½“å‰ Loss ç´¯è®¡å¹³å‡ Loss
# 0 Â  Â  5.0 Â  Â  Â  Â  5.0
# 1 Â  Â  4.8 Â  Â  Â  Â  (5.0 + 4.8)/2 = 4.9
# 2 Â  Â  4.6 Â  Â  Â  Â  (5.0+4.8+4.6)/3 â‰ˆ 4.8
# ... Â  ... ...
# 99 Â  Â 2.0 Â  Â  Â  Â  â‰ˆ 3.5ï¼ˆè¿˜åœ¨ä¸‹é™ï¼‰
# 100 Â  1.5 Â  Â  Â  Â  â‰ˆ 3.45ï¼ˆç¼“æ…¢ä¸‹é™ï¼‰
# 200 Â  1.2 Â  Â  Â  Â  â‰ˆ 2.8
# 500 Â  1.0 Â  Â  Â  Â  â‰ˆ 2.0
# 1000 Â 1.0 Â  Â  Â  Â  â‰ˆ 1.5
# 2000 Â 1.0 Â  Â  Â  Â  â‰ˆ 1.25
```